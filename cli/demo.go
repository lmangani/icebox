package cli

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/TFMV/icebox/catalog"
	"github.com/TFMV/icebox/config"
	"github.com/TFMV/icebox/importer"
	"github.com/apache/iceberg-go"
	"github.com/apache/iceberg-go/table"
	"github.com/spf13/cobra"
)

var demoCmd = &cobra.Command{
	Use:   "demo",
	Short: "Set up demo datasets for exploring Iceberg features",
	Long: `Set up demo datasets to quickly explore Apache Iceberg features.

This command creates sample namespaces and imports demo datasets including:
- NYC Taxi data for analytics queries and partitioning examples
- Real-world data with temporal patterns for time-based operations
- Partitioned datasets demonstrating Iceberg's advanced capabilities

Perfect for exploring Iceberg's powerful features with realistic data!

Examples:
  icebox demo                        # Set up all demo datasets
  icebox demo --dataset taxi         # Set up only taxi data
  icebox demo --list                 # List available demo datasets`,
	RunE: runDemo,
}

type demoOptions struct {
	dataset string
	list    bool
	cleanup bool
	force   bool
	verbose bool
}

var demoOpts = &demoOptions{}

// DemoDataset represents a demo dataset configuration
type DemoDataset struct {
	Name        string            `json:"name"`
	Description string            `json:"description"`
	Namespace   string            `json:"namespace"`
	Table       string            `json:"table"`
	DataPath    string            `json:"data_path"`
	Partitioned bool              `json:"partitioned"`
	Properties  map[string]string `json:"properties"`
	Queries     []DemoQuery       `json:"sample_queries"`
}

// DemoQuery represents a sample query for demo datasets
type DemoQuery struct {
	Name        string `json:"name"`
	Description string `json:"description"`
	SQL         string `json:"sql"`
}

func init() {
	rootCmd.AddCommand(demoCmd)

	demoCmd.Flags().StringVar(&demoOpts.dataset, "dataset", "", "specific dataset to set up (taxi)")
	demoCmd.Flags().BoolVar(&demoOpts.list, "list", false, "list available demo datasets")
	demoCmd.Flags().BoolVar(&demoOpts.cleanup, "cleanup", false, "remove all demo datasets")
	demoCmd.Flags().BoolVar(&demoOpts.force, "force", false, "overwrite existing demo datasets")
	demoCmd.Flags().BoolVar(&demoOpts.verbose, "verbose", false, "show detailed progress")
}

// formatBytes converts bytes to human readable format
func formatDemoBytes(bytes int64) string {
	const unit = 1024
	if bytes < unit {
		return fmt.Sprintf("%d B", bytes)
	}
	div, exp := int64(unit), 0
	for n := bytes / unit; n >= unit; n /= unit {
		div *= unit
		exp++
	}
	return fmt.Sprintf("%.1f %cB", float64(bytes)/float64(div), "KMGTPE"[exp])
}

func runDemo(cmd *cobra.Command, args []string) error {
	// Find the Icebox configuration
	configPath, cfg, err := config.FindConfig()
	if err != nil {
		return fmt.Errorf("‚ùå Failed to find Icebox configuration\n"+
			"üí° Try running 'icebox init' first to create a new project: %w", err)
	}

	if demoOpts.verbose {
		fmt.Printf("Using configuration: %s\n", configPath)
	}

	// Handle list option
	if demoOpts.list {
		return listDemoDatasets()
	}

	// Handle cleanup option
	if demoOpts.cleanup {
		return cleanupDemoDatasets(cfg)
	}

	// Set up demo datasets
	return setupDemoDatasets(cfg)
}

func listDemoDatasets() error {
	datasets := getAvailableDatasets()

	fmt.Printf("üé¨ Available Demo Datasets:\n\n")

	for _, dataset := range datasets {
		fmt.Printf("üìä **%s**\n", dataset.Name)
		fmt.Printf("   Description: %s\n", dataset.Description)
		fmt.Printf("   Namespace: %s\n", dataset.Namespace)
		fmt.Printf("   Table: %s\n", dataset.Table)
		if dataset.Partitioned {
			fmt.Printf("   Partitioned: Yes\n")
		}
		fmt.Printf("   Sample Queries: %d\n", len(dataset.Queries))
		fmt.Printf("\n")
	}

	fmt.Printf("üí° Usage:\n")
	fmt.Printf("   icebox demo                           # Set up all datasets\n")
	fmt.Printf("   icebox demo --dataset taxi            # Set up specific dataset\n")
	fmt.Printf("   icebox demo --cleanup                 # Remove all demo data\n")

	return nil
}

func setupDemoDatasets(cfg *config.Config) error {
	// Create catalog
	cat, err := catalog.NewCatalog(cfg)
	if err != nil {
		return fmt.Errorf("‚ùå Failed to create catalog: %w", err)
	}
	defer cat.Close()

	// Create importer
	imp, err := importer.NewParquetImporter(cfg)
	if err != nil {
		return fmt.Errorf("‚ùå Failed to create importer: %w", err)
	}
	defer imp.Close()

	datasets := getAvailableDatasets()

	// Filter datasets if specific dataset requested
	if demoOpts.dataset != "" {
		filtered := make([]DemoDataset, 0)
		for _, ds := range datasets {
			if ds.Name == demoOpts.dataset {
				filtered = append(filtered, ds)
				break
			}
		}
		if len(filtered) == 0 {
			return fmt.Errorf("‚ùå Dataset '%s' not found. Use --list to see available datasets", demoOpts.dataset)
		}
		datasets = filtered
	}

	fmt.Printf("üé¨ Setting up demo datasets...\n\n")

	ctx := context.Background()

	for _, dataset := range datasets {
		if err := setupSingleDataset(ctx, cat, imp, dataset); err != nil {
			if demoOpts.force {
				fmt.Printf("‚ö†Ô∏è  Warning: Failed to setup %s: %v\n", dataset.Name, err)
				continue
			}
			return fmt.Errorf("‚ùå Failed to setup dataset %s: %w", dataset.Name, err)
		}
	}

	fmt.Printf("\n‚úÖ Demo setup complete!\n")
	fmt.Printf("\nüöÄ Try these commands:\n")

	for _, dataset := range datasets {
		fmt.Printf("\nüìä %s:\n", dataset.Name)
		for _, query := range dataset.Queries {
			fmt.Printf("   # %s\n", query.Description)
			fmt.Printf("   icebox sql \"%s\"\n", query.SQL)
		}
	}

	fmt.Printf("\nüí° Additional commands:\n")
	fmt.Printf("   icebox table list --namespace demo    # List demo tables\n")
	fmt.Printf("   icebox sql \"SHOW TABLES\"              # Show all available tables\n")
	fmt.Printf("   icebox demo --cleanup                 # Remove demo data\n")

	return nil
}

func setupSingleDataset(ctx context.Context, cat catalog.CatalogInterface, imp *importer.ParquetImporter, dataset DemoDataset) error {
	// Create namespace
	namespace := table.Identifier{dataset.Namespace}
	exists, err := cat.CheckNamespaceExists(ctx, namespace)
	if err != nil {
		return fmt.Errorf("failed to check namespace: %w", err)
	}

	if !exists {
		props := iceberg.Properties{
			"description": "Demo namespace for sample datasets",
			"created_by":  "icebox-demo",
		}
		if err := cat.CreateNamespace(ctx, namespace, props); err != nil {
			return fmt.Errorf("failed to create namespace: %w", err)
		}
		fmt.Printf("‚úÖ Created namespace: %s\n", dataset.Namespace)
	}

	// Check if table exists
	tableIdent := table.Identifier{dataset.Namespace, dataset.Table}
	tableExists, err := cat.CheckTableExists(ctx, tableIdent)
	if err != nil {
		return fmt.Errorf("failed to check table existence: %w", err)
	}

	if tableExists {
		if !demoOpts.force {
			fmt.Printf("‚ö†Ô∏è  Table %s.%s already exists (use --force to overwrite)\n", dataset.Namespace, dataset.Table)
			return nil
		}

		// Drop existing table
		if err := cat.DropTable(ctx, tableIdent); err != nil {
			return fmt.Errorf("failed to drop existing table: %w", err)
		}
		fmt.Printf("üóëÔ∏è  Dropped existing table: %s.%s\n", dataset.Namespace, dataset.Table)
	}

	// For partitioned datasets, we need to handle them differently
	if dataset.Partitioned {
		return setupPartitionedDataset(ctx, cat, imp, dataset)
	}

	// For non-partitioned datasets, use single file import (legacy support)
	dataFile := dataset.DataPath
	absDataFile, err := filepath.Abs(dataFile)
	if err != nil {
		return fmt.Errorf("failed to get absolute path: %w", err)
	}

	// Import the dataset
	fmt.Printf("üì• Importing %s dataset...\n", dataset.Name)

	// Infer schema first
	schema, stats, err := imp.InferSchema(absDataFile)
	if err != nil {
		return fmt.Errorf("failed to infer schema: %w", err)
	}

	if demoOpts.verbose {
		fmt.Printf("   Schema: %d columns, %d rows, %s\n",
			len(schema.Fields), stats.RecordCount, formatDemoBytes(stats.FileSize))
	}

	// Import table
	result, err := imp.ImportTable(ctx, importer.ImportRequest{
		ParquetFile:    absDataFile,
		TableIdent:     tableIdent,
		NamespaceIdent: namespace,
		Schema:         schema,
		Overwrite:      demoOpts.force,
		PartitionBy:    nil,
	})
	if err != nil {
		return fmt.Errorf("failed to import dataset: %w", err)
	}

	fmt.Printf("‚úÖ Imported %s: %d records, %s\n",
		dataset.Name, result.RecordCount, formatDemoBytes(result.DataSize))

	return nil
}

func setupPartitionedDataset(ctx context.Context, cat catalog.CatalogInterface, imp *importer.ParquetImporter, dataset DemoDataset) error {
	fmt.Printf("üì• Setting up partitioned dataset %s...\n", dataset.Name)

	// Resolve the actual data path (check multiple locations)
	actualDataPath, err := resolveDemoDataPath(dataset.DataPath)
	if err != nil {
		return fmt.Errorf("failed to locate demo data: %w", err)
	}

	// Find all parquet files in the demo directory
	parquetFiles, err := findParquetFiles(actualDataPath)
	if err != nil {
		return fmt.Errorf("failed to find parquet files: %w", err)
	}

	if len(parquetFiles) == 0 {
		return fmt.Errorf("no parquet files found in %s", actualDataPath)
	}

	if demoOpts.verbose {
		fmt.Printf("   Found %d parquet files in %s\n", len(parquetFiles), filepath.Base(actualDataPath))
	}

	// For the demo, we'll use the first file to create a representative dataset
	// This avoids partitioning complexities while providing a working demo
	firstFile := parquetFiles[0]

	// Use the first file to infer schema
	schema, stats, err := imp.InferSchema(firstFile)
	if err != nil {
		return fmt.Errorf("failed to infer schema from first file: %w", err)
	}

	if demoOpts.verbose {
		fmt.Printf("   Schema: %d columns, %d rows, %s\n",
			len(schema.Fields), stats.RecordCount, formatDemoBytes(stats.FileSize))
	}

	tableIdent := table.Identifier{dataset.Namespace, dataset.Table}
	namespace := table.Identifier{dataset.Namespace}

	// Import the first file as a single table (more reliable for demo purposes)
	result, err := imp.ImportTable(ctx, importer.ImportRequest{
		ParquetFile:    firstFile,
		TableIdent:     tableIdent,
		NamespaceIdent: namespace,
		Schema:         schema,
		Overwrite:      demoOpts.force,
		PartitionBy:    nil, // Don't partition for demo to avoid metadata issues
	})
	if err != nil {
		return fmt.Errorf("failed to import demo dataset: %w", err)
	}

	fmt.Printf("‚úÖ Imported demo dataset %s: %d records, %s (from %s)\n",
		dataset.Name, result.RecordCount, formatDemoBytes(result.DataSize), filepath.Base(firstFile))

	if len(parquetFiles) > 1 {
		fmt.Printf("‚ÑπÔ∏è  Note: Demo uses 1 of %d available files - perfect for exploring Iceberg features!\n", len(parquetFiles))
	}

	return nil
}

func resolveDemoDataPath(basePath string) (string, error) {
	// Try multiple potential locations for the demo data
	possiblePaths := []string{
		basePath,                          // Current directory
		"../" + basePath,                  // Parent directory (when in project subdirectory)
		"../../" + basePath,               // Two levels up
		filepath.Join("icebox", basePath), // In icebox subdirectory
	}

	for _, path := range possiblePaths {
		if _, err := os.Stat(path); err == nil {
			// Check if this path actually contains parquet files
			if hasParquetFiles(path) {
				absPath, err := filepath.Abs(path)
				if err != nil {
					continue
				}
				return absPath, nil
			}
		}
	}

	return "", fmt.Errorf("demo data directory not found (tried: %v)", possiblePaths)
}

func hasParquetFiles(dirPath string) bool {
	// Use filepath.Walk to recursively search for parquet files
	hasFiles := false
	err := filepath.Walk(dirPath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return nil // Continue despite errors
		}
		if !info.IsDir() && strings.HasSuffix(strings.ToLower(info.Name()), ".parquet") {
			hasFiles = true
			return filepath.SkipDir // Stop searching once we find one
		}
		return nil
	})
	return err == nil && hasFiles
}

func findParquetFiles(dataPath string) ([]string, error) {
	var parquetFiles []string

	err := filepath.Walk(dataPath, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}

		if !info.IsDir() && strings.HasSuffix(strings.ToLower(info.Name()), ".parquet") {
			absPath, err := filepath.Abs(path)
			if err != nil {
				return err
			}
			parquetFiles = append(parquetFiles, absPath)
		}

		return nil
	})

	return parquetFiles, err
}

func cleanupDemoDatasets(cfg *config.Config) error {
	// Create catalog
	cat, err := catalog.NewCatalog(cfg)
	if err != nil {
		return fmt.Errorf("‚ùå Failed to create catalog: %w", err)
	}
	defer cat.Close()

	ctx := context.Background()
	namespace := table.Identifier{"demo"}

	// Check if demo namespace exists
	exists, err := cat.CheckNamespaceExists(ctx, namespace)
	if err != nil {
		return fmt.Errorf("‚ùå Failed to check namespace: %w", err)
	}

	if !exists {
		fmt.Printf("üì≠ No demo datasets found to clean up\n")
		return nil
	}

	fmt.Printf("üßπ Cleaning up demo datasets...\n")

	// List and drop all tables in demo namespace
	var tables []table.Identifier
	for identifier, err := range cat.ListTables(ctx, namespace) {
		if err != nil {
			return fmt.Errorf("‚ùå Failed to list tables: %w", err)
		}
		tables = append(tables, identifier)
	}

	for _, tableIdent := range tables {
		if err := cat.DropTable(ctx, tableIdent); err != nil {
			fmt.Printf("‚ö†Ô∏è  Failed to drop table %v: %v\n", tableIdent, err)
		} else {
			fmt.Printf("üóëÔ∏è  Dropped table: %s\n", strings.Join(tableIdent, "."))
		}
	}

	// Drop namespace
	if err := cat.DropNamespace(ctx, namespace); err != nil {
		return fmt.Errorf("‚ùå Failed to drop demo namespace: %w", err)
	}

	fmt.Printf("‚úÖ Demo cleanup complete!\n")
	return nil
}

func getAvailableDatasets() []DemoDataset {
	return []DemoDataset{
		{
			Name:        "taxi",
			Description: "NYC Taxi trip data with partitioning by year and month - perfect for analytics",
			Namespace:   "demo",
			Table:       "nyc_taxi",
			DataPath:    "demo", // Will be resolved to correct path
			Partitioned: true,
			Properties: map[string]string{
				"data.source":      "demo",
				"data.type":        "taxi",
				"data.partitioned": "true",
				"data.location":    "NYC",
				"data.format":      "parquet",
			},
			Queries: []DemoQuery{
				{
					Name:        "count_trips",
					Description: "Count total number of taxi trips",
					SQL:         "SELECT COUNT(*) as total_trips FROM demo.nyc_taxi",
				},
				{
					Name:        "average_fare",
					Description: "Calculate average fare amount",
					SQL:         "SELECT AVG(fare_amount) as avg_fare, AVG(total_amount) as avg_total FROM demo.nyc_taxi WHERE fare_amount > 0",
				},
				{
					Name:        "trips_by_month",
					Description: "Analyze trips by month (temporal analysis)",
					SQL:         "SELECT DATE_TRUNC('month', pickup_datetime) as month, COUNT(*) as trips, AVG(trip_distance) as avg_distance FROM demo.nyc_taxi GROUP BY month ORDER BY month",
				},
				{
					Name:        "payment_methods",
					Description: "Analyze payment method distribution",
					SQL:         "SELECT payment_type, COUNT(*) as count, AVG(tip_amount) as avg_tip FROM demo.nyc_taxi GROUP BY payment_type ORDER BY count DESC",
				},
				{
					Name:        "vendor_analysis",
					Description: "Compare taxi vendors by performance",
					SQL:         "SELECT vendor_name, COUNT(*) as trips, AVG(fare_amount) as avg_fare, AVG(trip_distance) as avg_distance FROM demo.nyc_taxi WHERE vendor_name IS NOT NULL GROUP BY vendor_name",
				},
				{
					Name:        "busy_times",
					Description: "Find busiest pickup hours",
					SQL:         "SELECT EXTRACT(hour FROM pickup_datetime) as hour, COUNT(*) as trips FROM demo.nyc_taxi GROUP BY hour ORDER BY hour",
				},
			},
		},
	}
}
